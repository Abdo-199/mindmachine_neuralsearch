{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "qa_model = pipeline(\"question-answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from PdfReader import pdf_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForQuestionAnswering(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.5845552086830139,\n",
       " 'start': 2316,\n",
       " 'end': 2351,\n",
       " 'answer': 'eight Nvidia A 100 Tensor Core GPUs'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the technical specification of the high performance cluster of the KI-Werkstatt?\"\n",
    "context = \"Python along with the framework PyTorch. Originally created by Meta AI and now a part of the Linux Foundation, PyTorch is a machine learning framework built on the Torch library and used for applications like computer vision and natural language pro- cessing. Instead of writing all the math require d for training and processing the data manually in Python, PyTorch makes it easier by providing many functions that can be used to make writing ML code easier and faster. It also has some pretrained models, and datasets built in [15]. Two main machines were used in this project to write and execute the code and to collect the data, a PC at XION and the HPC cluster of the KI -Werkstatt. The PC at XION was mainly for collecting the data from the XION ’s archive , disassembling the videos into frames, storing the images in the base directory structure of the datasets , preprocessing the images, and creating datasets . The cluster was used for creating data loaders, building the models, training the models, logging the experiences results into WandB platform, and evaluating the results. WandB is an MLOps platform , which helps the ML developers to easily track their experiments , collaborate with their teams and manage data and artifacts. It saves all the work of creating plots and plotting them together in a big notebook, instead the results of the experiments can be logged in WandB and access ed later in the web browser in a well-organized UI. It also organizes the models and the datasets as artifacts to keep an overview of which model and dataset are used for this particular experiment [16]. The IDE used was visual studio code. The wide range of extensions it offers makes the coding experience easier and more convenient. Python, Pylance, and Jupyter extensions were the core of running and debugging the Jupyter notebooks and the python scripts. On the other hand, remote explorer and Remote – SHH enabled the connection with the HPC cluster. After configuring the SSH connection between my PC and the cluster and adding the connec- tion to the remote explorer in vs -code, the terminal and the file tree of the cluster were acces- sible in the vs code window. This made the experience of using the high computing power of the cluster more convenient. The HPC of KI-Werkstatt is equipped with eight Nvidia A 100 Tensor Core GPUs . The A100 can be partitioned into seven GPU instances and offers up to 20X more performance than the previous generation, allowing it to adapt flexibly to changing needs in ML field . To handle the biggest models and datasets, the A100 80GB introduces the fastest memory bandwidth at over 2 teraby tes per second (TB/s) [17]. 10 To summarize the setup as a pipeline, the process involves the following steps: 1. Obtaining the data from the archive on the XION’s PC. 2. Organizing and preparing the data, to fit the file structure of the PyTorch `ImageFolder` function. 3. Uploading the prepared data to the cluster. 4. Creating\"\n",
    "qa_model(question = question, context = context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The HPC of KI-Werkstatt is equipped with eight Nvidia A 100 Tensor Core GPUs\n"
     ]
    }
   ],
   "source": [
    "def get_full_sentence(input_string, start_index, end_index):\n",
    "    \n",
    "    # going backwards from start_index, until an end mark is found\n",
    "    while start_index > 0 and input_string[start_index - 1] not in ['.', '!', '?']:\n",
    "        start_index -= 1\n",
    "    \n",
    "    # Find the end of the sentence (going forwards from end_index)\n",
    "    while end_index < len(input_string) - 1 and input_string[end_index + 1] not in ['.', '!', '?']:\n",
    "        end_index += 1\n",
    "    \n",
    "    full_sentence = input_string[start_index:end_index]\n",
    "    \n",
    "    return full_sentence\n",
    "\n",
    "# Example usage\n",
    "input_string = \"This is a long string. It contains multiple sentences! Here's another one?\"\n",
    "start_index = 2316 \n",
    "end_index = 2351    \n",
    "\n",
    "result = get_full_sentence(context, start_index, end_index)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trying passing the full document to the model\n",
    "it works fine, but takes too long time, so a compination of scentence transformer and bert is a good option initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pdf_to_text(\"D:\\\\Master\\\\1.Semester\\\\Software\\\\Dataset\\\\PDFS\\\\English\\\\Thesis\\\\Bachelorarbit_Elsharkawi_577909.pdf\", 10000000000 , False, 0)['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.5113534331321716,\n",
       " 'start': 32371,\n",
       " 'end': 32408,\n",
       " 'answer': 'eight Nvidia A 100  Tensor Core  GPUs'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_model(question, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## passing two documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pdf_to_text(\"D:\\\\Master\\\\1.Semester\\\\Software\\\\Dataset\\\\PDFS\\\\English\\\\Thesis\\\\Bachelorarbit_Elsharkawi_577909.pdf\", 10000000000 , False, 0)['text']\n",
    "text+= pdf_to_text(\"D:\\\\Master\\\\1.Semester\\\\Software\\\\Dataset\\\\PDFS\\\\English\\\\Uni\\\\RStPO_3AEO_EN.pdf\", 10000000000 , False, 0)['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.5113534331321716,\n",
       " 'start': 32371,\n",
       " 'end': 32408,\n",
       " 'answer': 'eight Nvidia A 100  Tensor Core  GPUs'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_model(question, text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
